#Полная информация про курс

# Фундаментальные концепции ИИ 2024
Страница курса "ФКИИ" [IT магистратур МАИ](http://itmai.ru/).

Автор :grinning:: `Вадим Кондаратцев`

- :email: vadim@mai-ai-lab.ru
- tg: [@Winer_Hopf](https://t.me/Winer_Hopf)

Время занятий :clock1230:
- 18:15 - 21:30 понедельник

Место проведения занятий:airplane: 
- МАИ, ГУК А, IT этаж, IT-5

Формат зачета:mortar_board: 
- Экзамен или зачет с оценкой на основе выполенных лабораторных работ + экзаменационный билет с теорией.

Вопросы к экзамену:page_facing_up:
- I семестр: [теория](https://docs.google.com/document/d/1q0d5rToVaJArhrALutL817cQXEFvb-NP57D2dypzz4E/edit?usp=sharing)

 ---

### Полезные ссылки и материалы
- [Облако с учебниками, книжками и презентациями](https://disk.yandex.ru/d/CLLfd0DOWRzXzA)
- [Хендбук Яндекс Академии по ML](https://academy.yandex.ru/handbook/ml)
- [ML Mindmap](https://whimsical.com/machine-learning-roadmap-2020-CA7f3ykvXpnJ9Az32vYXva)
- [paperswithcode.com](https://paperswithcode.com/)
- [Курс по ML от ODS](https://habr.com/ru/company/ods/blog/322626/)
- [Курс по DL от MIT](http://introtodeeplearning.com/)

---

### Лекции
1. Введение в ИИ и машинное обучени
   1. Что есть ИИ, в чеи отличие AI, ML, DL
   2. Что такое биологические модели нейронов и зачем они нужны
   3. Какие есть основные источники для изучения машинного обучения
   4. Формальная постановка задачи машинного обучения 
   5. Материалы
      1. [Презентация с лекции](http://www.machinelearning.ru/wiki/images/f/fc/Voron-ML-Intro-slides.pdf)
      2. [Статья про проблемы современного машинного обучения](https://habr.com/ru/companies/ods/articles/651103/)
2. Введение в оптимизацию. Градиентный спуск
   1. Таксономия разделов теории оптимизации
   2. Градиентный спуск
   3. Стохастический градиентный спуск
   4. Моментные модицикации
   5. Адаптивные модификации
   6. Управление темпом обучения
   7. Проблемы сорвеменного машинного обучения с точки зрения оптимизации 
   8. Материалы
      1. [презентация с лекции](http://www.machinelearning.ru/wiki/images/5/53/Voron-ML-Lin-SG.pdf)
      2. [Статья про проблемы современного машинного обучения (оптимизация)](https://habr.com/ru/articles/351924/)
      3. [Про методы оптимизации в Я.Хендбуке](https://academy.yandex.ru/handbook/ml/article/optimizaciya-v-ml)
      4. [Про модификации градиентного спуска на хабре](https://habr.com/ru/articles/318970/)
      5. [Условие Каруша-Куна-Таккера](https://ru.wikipedia.org/wiki/Условия_Каруша_—_Куна_—_Таккера)
3. Метаэвристическая глобальная оптимизация.
   1. Глобальная оптимизация и эвристики
   2. Генетические алгоритмы
   3. Устройство фреймворка [pygmo](https://esa.github.io/pygmo2/index.html)

---

### Лабораторные работы

1. Градиентный спуск и его модификации
   - Выбрать [тестовые функции оптимизации](https://ru.wikipedia.org/wiki/Тестовые_функции_для_оптимизации) (2 шт)
   - Запрограммировать собственнуб реализацию классического градиентного спуска
   - Запрограммировать пайлайн тестирования алгоритма оптимизации
     - Визуализации функции и точки оптимума
     - Вычисление погрешности найденного решения в сравнение с аналитическим для нескольких запусков
     - Визуализации точки найденного решения (можно добавить анимацию на плюс балл)
   - Запрограммировать метод вычисления градиента
     - Передача функции градиента от пользователя
     - Символьное вычисление градиента (например с помощью [sympy](https://www.sympy.org/en/index.html)) (на доп балл)
     - Численная аппроксимация градиента (на доп балл)
   - Запрограммировать одну моментную модификацию и протестировать ее
   - Запрограммировать одну адаптивную модификацию и протестировать ее
   - Запрограммировать метод эфолюции темпа обучения и/или метод выбора начального приближения и протестировать их
   - `Will be unclocked afetr Lecture №5`
  2. Глобальная оптимизация и метаэврестические алгоритмы
     1. В Pygmo запрогроммировать две своих тестовых функции и найти их оптимум 3 разными алгоритмами доступными в библиотеке и получить таблицу сравнения
  3. Оптимизация гиперпараметров 
     1. С помощью [optuna]() взять пример, аналогичный третьему туториалу документации, используя sklearn и с другим датасетом, выбрать другие  алгоритмы классификации и клстеризации не из туториала  и визуализировать графики для полученного процесса
        1. В качестве других моделей подойдут любые алгоритмы классификации и регрессии из sklearn которые не использовались в туториале
     2. Использовать 2 разных семплера и прунера
     3. При процессе оптимизации гиперпараметров использовать общую память через postgreSQL
     4. В качестве отчёта выступают: исходный код, инструкция запуска реляционной БД. 
  4. Восстановление функции распренделения вероятности
     1. Реализовать метод восстановления плотности вероятности двумя способами:
        1. EM-алгоритм
        2. Ядерное сглаживание
     2. Применить данные методы на любом наборе случайных точек
   5. Реализовать метод Метрополиса-Гастингса и Гибсона для несимметричного распределения. Применить два метода на основе той функции плотности, которая была восстановлена в прошлом пункте, тем самым получив изначальные точки.
     4. в методе М-Г нарисовать картинку блуждания в случае 3D-функции плотности (на доп баллы)
     5. Сравнить красный и синий набор точек (сгенерированые и исходные) (с помощью Расстояния Кульбака-Лейблера)
